---
layout:     post
title:      "HKU notebook - deep learning"
subtitle:   ""
date:       2022-05-11 22:00:00
updatedate:
author:     "Jpy"
header-img: "img/post-bg-2015.jpg"
no-catalog: False
onTop: false
latex: false
# ç”Ÿæ´»ï¼Œå·¥ä½œï¼Œç¬”è®°ï¼ˆä¸ªäººç†è§£æ¶ˆåŒ–å¿ƒå¾—ï¼‰ï¼Œæ–‡æ¡£ï¼ˆæ–¹ä¾¿åç»­æŸ¥é˜…çš„èµ„æ–™æ•´ç†ï¼‰ï¼Œé¡¹ç›®ï¼Œå…¶ä»–
tags:
    - ç¬”è®°
---

# 1A Motivation

P38 what triggered Deep Learning's success in recent years?

# 1B Linear Models

P6 when should you use ML?

P15 Linear Classification

P51 Logistic Regression

P70 multiclass classification & loss function

# 2A Artificial Neural Networks

P10 Perceptron

P13 activation functions

# 2B Artificial Neural Networks

P8 Backpropagation
$$
s_j^{(l)}=(\sum_{i}w_{ij}^{(l)}x_i^{(l-1)})+b \\
x_j^{(l)}=g(s_J^{(l)})
$$
P36 earlier layers æ±‚å¯¼è¿‡ç¨‹

ç¬¬iå±‚çš„æ¢¯åº¦æ±‚æ³•
$$
w_{ij}^{(l)} \rightarrow w_{ij}^{(l)} - \alpha x_i^{(l-1)} \delta _j^{(l)} \\
\frac{\partial J(W)}{\partial w_{ij}^{(l)}}=x_i^{(l-1)}\delta_{j}^{(l)} \\
\delta_{i}^{(l)} =  g'(s_i^{(l)})(\sum_j w_{ij}^{(l+1)}\delta_j^{(l+1)})
$$
P41 Output Layer æ±‚å¯¼è¿‡ç¨‹ï¼Œå¸¸è§**æŸå¤±å‡½æ•°**æ±‚å¯¼
$$
w_{ij}^{(l)} \rightarrow w_{ij}^{(l)} - \alpha x_i^{(l-1)} \delta _j^{(l)} \\
\frac{\partial J(W)}{\partial w_{ij}^{(l)}}=x_i^{(l-1)}\delta_{j}^{(l)} \\
\delta_i^{(l)} = g'(s_i^{(L)})(\frac{\partial J}{\partial h(x)})
$$

# 3A Techniques to Improve Training

P10 æ¢¯åº¦æ¶ˆå¤±

P14 æ¿€æ´»å‡½æ•° not zero-centered ç¼ºç‚¹

P16 Sigmoid, Tanh, ReLUä¼˜ç¼ºç‚¹

P21 Problem with initial Symmetry

P23 ä¼˜åŒ–å™¨

P43 split dataset

# 3B Further Techniques to Improve Training

regularization

dropout

early stopping

P32 Bias and Variance

# 4A CNN and Computer Vision

P29 CVå‘å±•å²

P57 Convolutional Layer

P66 Padding

P79 è®¡ç®—parametersä¸ªæ•°ï¼ˆCNNåŠdense layerï¼‰

P84 SIRENs

P89 why pooling

P102 è®¡ç®—ç»´åº¦ï¼Œconvä¸pooling

convï¼Œå·ç§¯åçš„ç»´åº¦ä¸º$$M \times M \times filter\_num$$
$$
M = ((N+2*padding)-F)/S + 1
$$
poolingä¸convè®¡ç®—æ–¹å¼ä¸€è‡´ï¼Œåªä¸è¿‡ä¸æ”¹å˜é€šé“æ•°ç»´åº¦

P127 GoogleNetï¼›è®¡ç®—operationsï¼›1*1å·ç§¯çš„ä½œç”¨, Inception

# 5A CNN Visualization+application

**P24 25 ä¸¤ä¸ªexam**

P34 Saliencyï¼ˆæ˜¾è‘—æ€§ï¼‰ map and heat mapï¼Œå…·ä½“è¿‡ç¨‹å¯è§P48

P57 Object Localization

P64 Object Detection

P79 è¯­ä¹‰åˆ†å‰²

# 5B DeepDream Adversarial Images

# 6A Adversarial Images

P22 Autoencoders

# 6B Deep Generative Models

P19 VAEs. given a distribution by comparing its input to its output 

VAE ä¸ Autoencoderçš„åŒºåˆ«

P25 GAN

P38 when stop training

P39 Loss Function for Discriminator and Generator
$$
J = -[y \log h(x)+(1-y)\log (1-h(x))]
$$
Discriminator wants to minimize J, Generator wants to maximize J.

P42 Training of GAN

P43 Best Discriminator

P45 Problems in GAN

P58 VAEGAN C-GAN Pix2PixGAN

P66 ä¸åŒçš„å‡ ç§generatorçš„æ¶æ„ï¼›UNET Generator

P72 CycleGAN

# 7A RNNs

P43 RNNå…¬å¼

P46 Pros and Cons of typical RNN

P69 Backpropagation in RNN

P75 LSTM
$$
\begin{gathered}
i_{t}=\sigma\left(W_{i} X_{t}+U_{i} h_{t-1}+b_{i}\right), \\
f_{t}=\sigma\left(W_{f} X_{t}+U_{f} h_{t-1}+b_{f}\right), \\
o_{t}=\sigma\left(W_{o} X_{t}+U_{o} h_{t-1}+b_{o}\right), \\
\tilde{c}_{t}=\operatorname{Tanh}\left(W_{c} X_{t}+U_{c} h_{t-1}\right), \\
c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t}, \\
h_{t}=o_{t} \odot \operatorname{Tanh}\left(c_{t}\right) .
\end{gathered}
$$
P88 Bidirectional RNN

# 8 Machine Translation

P32 Machine Translation

P45 Alignment Probability

P57 Document vector

P65 Word2Vec

P74 Glove

P92 Evaluation of Word Vectors

P111 Transformer

# 9A Machine Translation(2)

P19 BERT

P23 Back translation

# 9B Deep Reinforcement Learning

P11 Reinforcement Learning

P32 Valued-based Learning å­¦ä¹ æ¯ä¸ªä½ç½®çš„valueï¼Œæ ¹æ®valueå¾—åˆ°æœ€ä¼˜$$\pi$$
$$
\begin{aligned}
Q^{*}\left(s_{t}, a_{t}\right) &=E\left[r_{t}+\gamma \max _{a^{\prime}} Q^{*}\left(s_{t+1}, a^{\prime}\right)\right] \\
&=\sum_{s^{\prime}, r^{\prime}} P\left(s^{\prime}, r^{\prime} \mid s_{t}, a_{t}\right)\left[r^{\prime}+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right] \\

v(s_t) &= \sum_{s'}\pi(s'|s_t) Q^{\pi}(s_t,a) \\

v^{*}\left(s_{t}\right) &=\max _{a} Q^{*}\left(s_{t}, a\right) \\
&=\max _{a} \sum_{s^{\prime}, r^{\prime}} P\left(s^{\prime}, r^{\prime} \mid s_{t}, a\right)\left[r^{\prime}+\gamma v^{*}\left(s^{\prime}\right)\right]
\end{aligned} \\

$$
P36 Policy Learning

Value Iterationï¼Œæ ¹æ®maxå‡½æ•°è®¡ç®—å‡ºæ¯ä¸ªä½ç½®è¿›è¡Œæ¯ä¸ªåŠ¨ä½œçš„Qå€¼ï¼Œé€šè¿‡æ‰¾åˆ°maxçš„Qå€¼ä½œä¸ºæ–°çš„Valueå€¼ã€‚

Policy Iterationåˆ†ä¸ºä¸¤æ­¥ï¼Œå…ˆæ ¹æ®ä¹‹å‰å¾—åˆ°çš„policyï¼Œä¸€ä¸ªå›ºå®šçš„è·¯å¾„ï¼Œæ›´æ–°æ‰€æœ‰æ ¼å­çš„valueå€¼ã€‚ç¬¬äºŒæ­¥ï¼Œå†æ ¹æ®ä¸åŒçš„Valueå€¼æ›´æ–°Policy

![img](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20211204231128974.png)

P45 Q-learning

Exploitation: with probability 1-e use greedy action, i.e., ğ‘" = arg maxQ( s_t, ğ‘)
Exploration: with probability e use random action
$$
Q(s, a) \leftarrow {Q(s, a)+\alpha}\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right]
$$
P50 Deep Q-Network

P74 Double DQN

# 10A Deep Reinforcement Learning

# 10B Ethical Issues in AI