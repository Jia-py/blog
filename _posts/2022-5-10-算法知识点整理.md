---
layout:     post
title:      "算法知识点整理"
subtitle:   ""
date:       2022-05-10 22:00:00
updatedate:
author:     "Jpy"
header-img: "img/post-bg-2015.jpg"
no-catalog: False
onTop: false
latex: false
# 生活，工作，笔记（个人理解消化心得），文档（方便后续查阅的资料整理），项目，其他
tags:
    - 笔记
---

# 基础数据结构

## 链表与数组的优缺点：

数组：

优点：使用方便，查询效率比链表高，内存为一连续的区域

缺点：大小固定，不适合动态存储，不方便动态添加

链表：

优点：可动态添加删除，大小可变

缺点：只能通过顺次指针访问，查询效率低

## 排序算法

**排序算法的稳定的意思是说，相同大小的元素在数组中的相对位置是稳定的。**

### 选择排序 Selection Sort

每次选最小的放到最前面，时间复杂度$$O(n^2)$$

```python
def selection_sort(nums):
    n = len(nums)
    for i in range(n):
        min_idx = i
        for j in range(i,n):
            if nums[j] < nums[min_idx]:
                min_idx = j
        nums[i],nums[min_idx] = nums[min_idx],nums[i]
    return nums
```

### 冒泡排序 Bubble Sort

每次将大的放后面，小的放前面，时间复杂度$$O(n^2)$$，稳定排序。相当于每次稳定找出最大值，放在当前遍历的最后。

```python
def bubble_sort(nums):
    n = len(nums)
    for i in range(n):
        for j in range(1,n-i):
            if nums[j-1] > nums[j]:
                nums[j-1],nums[j] = nums[j], nums[j-1]
    return nums
```

### 插入排序 Insertion Sort

保证前i的部分是有序的，每次循环将当前元素插入到前面的有序部分中，时间复杂度$$O(n^2)$$，稳定排序。

```python
def insertion_sort(nums):
    n = len(nums)
    for i in range(1,n):
        # 从i位置往前找插入位置
        while i > 0 and nums[i-1] > nums[i]:
            nums[i-1],nums[i] = nums[i],nums[i-1]
            i -= 1
    return nums
```

### 希尔排序 Shell Sort

使用多次不同的间隔排序子序列，直到间隔降为1，排序整个数组。希尔排序的时间复杂度和增量序列（间隔的选择序列）是相关的。这里代码中使用的`{1,2,4,8,...}`这种序列并不是很好的增量序列，使用这个增量序列的时间复杂度最坏情况是$$O(n^2)$$，稳定排序。

```python
def shell_sort(nums):
    n = len(nums)
    gap = n//2
    while gap:
        for i in range(gap,n):
            while i-gap >=0 and nums[i-gap] > nums[i]:
                nums[i-gap],nums[i] = nums[i],nums[i-gap]
                i -= gap
        gap //= 2
    return nums
```

`Hibbard`提出了另一个增量序列$$1,3,7,...,2^k-1$$，这种序列的时间复杂度最坏为$$O(n^{1.5})$$

`Sedgewick`提出了几种增量序列，其最坏情况为$$O(n^{1.3})$$，其中最好的一个序列`{1,5,19,41,109}`

### 归并排序 Merge Sort

分治法，将数组分成子序列，让子序列有序，再将子序列合并。

稳定排序，外排序（占用额外内存），时间复杂度$$O(nlogn)$$

```python
def merge_sort(nums):
    if len(nums) <= 1:
        return nums
    mid = len(nums) // 2
    # 分
    left = merge_sort(nums[:mid])
    right = merge_sort(nums[mid:])
    # 合并
    return merge(left, right)


def merge(left, right):
    res = []
    i = 0
    j = 0
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            res.append(left[i])
            i += 1
        else:
            res.append(right[j])
            j += 1
    res += left[i:]
    res += right[j:]
    return res
```

### 快速排序 Quick Sort

选取pivot，递归排序。不稳定排序，时间复杂度$$O(nlogn)$$。

代码的这种写法简洁易懂，但需要额外的空间。

```python
def quicksort(arr):
    if not arr:
        return []
    key = arr[0]
    # 取切片是不会报错的，即使len(arr)==1
    left = quicksort([i for i in arr[1:] if i < key])
    right = quicksort([i for i in arr[1:] if i >= key])
    return left + [key] + right
```

其他写法

[快速排序算法_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1at411T75o?from=search&seid=6628211241811756261&spm_id_from=333.337.0.0)

```python
def quickSort(nums: List[int], low: int, high: int):
    # low和high为该区间的左右两个端点的index
    if low < high:
        pivot = nums[low]
        i, j = low, high
        # 最后i和j会相等
        while i < j:
            # 找到从右往左第一个小于pivot的值
            while i < j and nums[j] >= pivot: j -= 1
            if i < j:
                nums[i] = nums[j]
                # i往右移动一位
                i += 1
            while i < j and nums[i] <= pivot: i += 1
            if i < j:
                nums[j] = nums[i]
                j -= 1
        nums[i] = pivot
        # 递归两边
        quickSort(nums, low, i - 1)
        quickSort(nums, i + 1, high)
quickSort(nums,0,len(nums)-1)
```

### 堆排序 Heap Sort

[排序算法：堆排序【图解+代码】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1fp4y1D7cj?spm_id_from=333.337.search-card.all.click)

```python
# 最大堆
# nums为待排序数组，end为待排序部分长度，i为当前待维护元素的下标
def heapify(nums,end,i):
    largest, lson, rson = i,2*i+1,2*i+2
    # 若lson比当前的root节点大，则需要交换
    if lson < end and nums[largest] < nums[lson]:
        largest = lson
    if rson < end and nums[largest] < nums[rson]:
        largest = rson
    # 若largest不等于i，即其左右子节点比他大了，已经被交换了
    if largest != i:
        nums[largest], nums[i] = nums[i], nums[largest]
        # 并且需要继续维护较大的子节点下的树
        heapify(nums, end, largest)
n = len(nums)
# 建堆，从i==n//2开始，即使得孩子节点为数组的末尾，保证从树的底部向上更新
for i in range(n//2,-1,-1):
    heapify(nums,n,i)
# 排序，每次保证数组末尾的有序，即每次取出最大的放到最后
for i in range(n-1,0,-1):
    nums[i], nums[0] = nums[0], nums[i]
    heapify(nums,i,0)
```

```python
# 堆的插入
# 在建堆后操作
nums.append(num)
n = len(nums)
# 计算要遍历哪些节点
i = n - 1
idx_lis = []
while i >= 0:
    if (i-1) // 2 >= 0:
        idx_lis.append((i-1)//2)
    i = (i-1) // 2
# 由底向顶遍历
for i in idx_lis:
    heapify(nums,n,i)
```



# 概率

1. 扑克牌54张，现分成三等份18张，问大小王同时出现在一堆内的概率

总共的分法有 $$C_{54}^{18} C_{36}^{18} C_{18}^{18}$$种，而我们先将两张大小王确定放在哪一堆有三种方法，再确定该堆的16张牌，以及其他堆的18张牌，共$$C_{3}^{1}C_{52}^{16}C_{36}^{18}C_{18}^{18}$$种，相除可得$$\frac{17}{53}$$

# 聚类

## K-means python实现

## K-means如何选取k值

计算不同的k值与对应的WSS（within cluster sum of squares）也就是各个点到cluster中心的距离的平方的和

选取鞍部的点

## DBSCAN原理

[DBSCAN 算法 - 简书 (jianshu.com)](https://www.jianshu.com/p/e594c2ce0ac0)

# 评估模型

## ROC

横坐标为假阳性率$$FPR=FP/(FP+TN)$$,代表假样本有多少被预测为了正样本。

纵坐标为真阳性率$$TPR=TP/(TP+FN)$$,其实就是recall。

一般二分模型都是概率模型，而ROC图像中不同的点对应的是不同阈值下的分类模型。左下角，即FPR=TPR=0的点为设置阈值为正无穷（针对分类为正类而言），而右上角，FPR=TPR=1的点为设置阈值为0的点。

什么是AUC？AUC（area under curve）指ROC曲线下的面积，面积越大，则分类器越可能把真正的正样本排在前面，分类性能越好。

ROC曲线与P-R曲线的区别：

1. ROC曲线形状基本保持不变，而P-R曲线形状一般会发生较剧烈的变化。可以应对实际场景中正负样本数量很不平衡的情况。

## 如何计算AUC

$$
\frac{\sum I\left(P_{\text {正样本 }}, P_{\text {负样本 }}\right)}{M^{*} N} \quad I\left(P_{\text {正样本 }}, P_{\text {负样本育 }}\right)=\left\{\begin{array}{l}
1, P_{\text {正样本 }}>P_{\text {正样本 }} \\
0.5, P_{\text {正样本 }}=P_{\text {负样本 }} \\
0, P_{\text {正样本 }}<P_{\text {负样本 }}
\end{array}\right.
$$

先搞清楚概念：AUC是指 随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值 比 分类器输出该负样本为正的那个概率值 要大的可能性。

M为实际为正样本的集合，N为实际为负样本的集合，对$$M \times N$$个样本对，计算$$I$$值的求和，除以$$M \times N$$即可。

# 机器学习基础

## 特征处理

| 类别特征处理   | one-hot编码，Multi-hot编码 |
| -------------- | -------------------------- |
| 数值型特征处理 | 归一化，分桶               |
| 序号特征编码   | 高：3，中：2，低：1        |

## 如何处理ID型特征

[推荐场景下的ID类特征处理方式 - 简书 (jianshu.com)](https://www.jianshu.com/p/5aea25e80520)

[Embedding从入门到专家必读的十篇论文 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/58805184)

## 如何评价Embedding

1. similarity, 相似度评价。扔两个相近的词进去，看embedding出来的向量相似度是不是比较高
2. 直接作为特征放到模型里跑，看有没有提升
3. word analogy, 假设给了一对单词 (a , b) 和一个单独的单词c, task会找到一个单词d，使得c与d之间的关系相似于a与b之间的关系，举个简单的例子：(中国，北京)和 日本，应该找到的单词应该是东京，因为北京是中国的首都，而东京也是日本的首都。 在给定word embedding的前提下，task一般是通过在词向量空间寻找离(b-a+c)最近的词向量来找到d。
4. visualization, 查看聚类分布效果

[CTR 预测理论（八）：Embedding 质量评估方法总结_dby_freedom的博客-CSDN博客_评估embedding](https://blog.csdn.net/Dby_freedom/article/details/88820726)

## 如何挑选特征

- Filter(过滤法)：按照`发散性`或`相关性`对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选（皮尔逊相关系数）
- Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征
- Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）。L1正则天然具有稀疏解的特性，具备特征选择的特性。

## K折交叉验证 k-fold 与留出法hold out对比

留出法会较大受到数据集分割的影响，而交叉验证可以优化这个问题。

## L1 L2对比

都是既可以用于损失函数，又可以用于正则化。

损失函数：

$$L1: \ S=\sum_{i=1}^{n}\left|y_{i}-f\left(x_{i}\right)\right|$$ 

$$ L2:\ S=\sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$$

一般损失函数用L2的原因是计算方便，可以直接求导获取最小值时各个参数的取值。L1鲁棒性更强，对异常值更不敏感。

L2范式有唯一最优解，而L1范式没有唯一解。

![](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220210184941527.png)

绿色的线是L2范式的唯一解，而其他三条线都可以是L1的最优解。

正则项：
$$
L1: \ \lambda \sum_{i=1}^{k}\left|w_{i}\right| \\
L2: \ \lambda \sum_{i=1}^{k} w_{i}^{2}
$$

* L2计算更方便，而L1在特别是非稀疏向量上的计算效率很低
* L1的输出是稀疏的，会把不重要的特征直接置零，这是因为在梯度更新时，不管 L1 的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。而L2距离0越近则步长越小。
* L2有唯一解，而L1没有唯一解。

<img src="https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220210185153408.png" alt="image-20220210185153408" style="zoom:50%;" />



## 梯度消失与梯度爆炸的原因与解决

可能原因：梯度消失：1. 深层网络 2. 不合适的损失函数，如选择了sigmoid激活函数，其导数最大为0.25，极易累乘导致梯度消失。 梯度爆炸：1. 深层网络 2. 权值初始化太大

解决方法：

梯度消失：1. 深度残差网络，引入残差学习的方式，使我们能学习到更深层的网络表示 2. LSTM及各种变种门控循环单元，弥补了梯度消失所带来的损失 3. batch norm

梯度爆炸：1. 梯度裁剪，当梯度大于某个给定值时，对梯度进行等比收缩 

能有效预防梯度爆炸，如果梯度长度超过$$\theta$$，就把长度投影回$$\theta$$
$$
\mathbf{g} \leftarrow \min \left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
$$

2. 通过正则化限制w不过大 
3. batch norm

## 处理过拟合和欠拟合

过拟合：模型过于复杂，把噪声数据的特征也学习到模型中，导致模型泛化能力下降。

降低过拟合和欠拟合的方法

* 降低过拟合的方法：
  * 获取更多的训练数据，是最有效的，直接获取数据困难，可以数据增强。
  * 降低模型复杂度
  * 正则化，比如将权值大小加入损失函数，避免权值过大带来过拟合。
  * 集成学习
  * dropout
* 降低欠拟合的方法：
  * 添加新特征
  * 增加模型复杂度
  * 减小正则化系数

## 样本不均衡的处理方法

数据方面：上采样，下采样

损失函数角度：增大样本少的类别的损失权重，减少样本多的类别的损失权重

其他：利用半监督或自监督学习解决样本不均衡

如何解决机器学习中样本不均衡问题？ - 数据拾光者的回答 - 知乎 https://www.zhihu.com/question/66408862/answer/1647753758

### 图像分类的损失函数为什么不采用mse

在分类问题中，使用sigmoid/softmx得到概率，配合MSE损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况。

![image-20220213220032829](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220213220032829.png)

# 模型

## 线性回归

[浅析机器学习：线性回归 & 逻辑回归 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/39363869)

## LR

## LR的loss

![image-20220214235912471](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220214235912471.png)

![image-20220214235921832](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220214235921832.png)
$$
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\left[y_{i} \cdot \log \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \log \left(1-p_{i}\right)\right]
$$

求导

![image-20220211013305045](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220211013305045.png)

### **LR连续特征离散化的好处**

- 模型稳定性和鲁棒性：离散后的特征能去除噪声，对异常值不再敏感
- 简化模型：降低过拟合
- 提高模型表达能力：单变量变成多个，每个有独立的权重，引入非线性，加大拟合
- 计算更快速：稀疏变量内积计算更方便
- 易于解释性

### LR和神经网络的初始权值可以都设为0吗

LR可以，神经网络不行。

LR的原因参考梯度公式，梯度依然是依赖于x且有变化的。 

神经网络所有权值为0，则每一层的各个神经元结果都一样，反向传播求得的梯度也一样，更新的w也一样，无法学习不同的特征。

### 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响

- 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
- 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。
- 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

### 为什么我们还是会在训练的过程当中将高度相关的特征去掉？

- 去掉高度相关的特征会让模型的可解释性更好
- 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。

### 线性回归与逻辑回归的区别和联系

联系

* 线性回归和逻辑回归都是**广义线性回归模型的特例**
* 二者在求解超参数的过程中，都可以使用梯度下降

区别

* 线性回归只能用于回归问题，逻辑回归用于分类问题
* 线性回归的因变量是连续的，而逻辑回归是离散的
* 线性回归无联系函数，逻辑回归的联系函数是对数几率函数，属于Sigmoid函数（与第一条的广义线性回归模型对应）
* 线性回归使用最小二乘法作为参数估计方法，可以认为是极大似然估计的简化，逻辑回归使用极大似然法作为参数估计方法。

## SVM原理

SVM推导，对偶问题，核函数及场景

### LR和SVM的联系区别

联系：

1. LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题
2. 两个方法都可以增加不同的正则化项，如L1L2。
3. LR和SVM都可以用来做非线性分类，只要加核函数就好
4. 在不考虑核函数的情况下，LR和SVM都是线性模型

区别：

1. 从目标函数来看，区别在于逻辑回归采用的是交叉熵损失函数，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
2. 逻辑回归模型更简单，好理解，大规模线性分类时比较方便。
3. SVM不直接依赖数据分布，而LR依赖，因为SVM只与支持向量有关系

## AdaBoost 随机森林 GBDT XGBoost对比

### AdaBoost

分类错误的样本在训练时，给更高的loss权重。

在最终分类器中，预测效果好的模型拥有更高权重，其实就相当于与loss权重成负相关。

![](https://img-blog.csdnimg.cn/2019052222433836.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xlbW9uNzU5NTk3,size_16,color_FFFFFF,t_70)

### 随机森林

bagging，与传统bagging只使用bootstrap有放回采样，随机森林为随机采样，防止过拟合能力更强。

如果有M个输入变量，每个节点都将随机选择m个特定的变量用于确定分裂点。在决策树的生成过程中，m的值是保持不变的。m一般取M均方根。

优点：

1. 能解决分类与回归问题，由于是集成学习，方差和偏差都比较低，泛化性能优越
2. 对于高维数据集的处理能力很好，被认为是不错的降维方法。此外，该模型能够输出特征的重要性程度，是一个非常实用的功能。
3. 可以应对缺失数据
4. 高度并行化，易于分布式实现
5. 树模型，不需要归一化即可之间使用

缺点

1. 忽略了属性之间的相关性
2. 随机森林在解决回归问题时并没有像在分类中表现的那么好，因为它不能给出一个连续型的输出。
3. 黑盒，只能调整不同参数和随机种子

![image-20220210213937777](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220210213937777.png)

### GBDT

[GBDT的原理、公式推导、Python实现、可视化和应用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/280222403)

GBDT中的树是CART回归树

提升树

---

![](https://pic3.zhimg.com/80/v2-fedb38d98fdc20eeaea35d966f085836_720w.jpg)

梯度提升

---

利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树。
$$
r_{m i}=-\left[\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f x)=f_{m-1}(x)}
$$
优点：

1. 灵活处理各类型的数据（连续、离散）
2. 易于特征组合、特征选择
3. 相对少调参，预测精度高
4. 使用决策树作为弱分类器，有较好的解释性与鲁棒性

缺点：

1. 串行生成，并行难
2. 数据纬度高，计算复杂度高，表现不如支持向量机或神经网络

### 比较LR和GBDT，什么情境下GBDT不如LR

- LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；

当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。主要原因是若某个特征凑巧在高维下在训练数据中表现较好，树模型会将其设为分裂点，而线性模型只是通过加大权值来计算。在有正则项的情况下，线性模型的权值会被限制不过大，但对于单个分裂点来说，树的正则项不能很好限制该错误分裂点。根本原因是正则项对两类模型的惩罚方式不同。

### GBDT VS Adaboost

GBDT与Adboost最主要的区别在于两者如何识别模型的问题。Adaboost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。GBDT通过负梯度来识别问题，通过计算负梯度来改进模型。

### GBDT VS RF

| 集成学习         | 都是集成学习方法，GBDT是Boosting思想，RF是Bagging思想    |
| ---------------- | -------------------------------------------------------- |
| 树的类型         | 都由多棵树组成，GBDT是CART回归树，RF分类树、回归树都可以 |
| 并行化（训练）   | GBDT只能顺序生成，RF的树可以并行生成                     |
| 优化指标（目标） | GBDT是偏差优化，RF是方差优化                             |
| 训练样本(数据)   | GBDT是每次全样本训练，RF有放回抽样训练                   |
| 最终结果（预测） | GBDT是多棵树累加之和，RF是多棵树进行多数表决             |

### XGBoost

[一篇文章搞定GBDT、Xgboost和LightGBM的面试 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/148050748)

[XGBoost 原理 及 常见面试题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/156047718)

基础补充：泰勒公式

一阶泰勒展开 :
$$
f(x) \approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)
$$
二阶泰勒展开 :
$$
f(x) \approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right)}{2 !}\left(x-x_{0}\right)^{2}
$$

### XGBoost对GBDT的优化：

1. 利用二阶泰勒公式展开：优化损失函数，提高计算精确度
2. 利用正则项：简化模型，避免过拟合
3. 采用Blocks存储结构：可以并行计算

结合GBDT的表示方法，我们可以将XGBoost的损失函数表示为
$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\sum_{k} \Omega\left(f_{k}\right)
$$
其中$$\sum_{k} \Omega\left(f_{k}\right)$$表示k棵树的复杂度

接下去三个步骤

1. **二阶泰勒展开，去除常数项，优化损失函数项**

对$$l(y_i,x)$$在$$\hat{y_i}^{t-1}$$处进行二阶泰勒展开得到
$$
l\left(y_{i}, x\right) \approx l\left(y_{i}, \widehat{y}_{i}^{(t-1)}\right)+l^{\prime}\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)\left(x-\widehat{y}_{i}^{(t-1)}\right)+\frac{l^{\prime \prime}\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)}{2}\left(x-\widehat{y}_{i}^{(t-1)}\right)^{2}
$$
![](https://pic3.zhimg.com/80/v2-be0c556bb9bba72a69926cb71c0f1d76_720w.jpg)

2. **正则化项展开，去除常数项，优化正则化项**

![](https://pic1.zhimg.com/80/v2-830f8bf08e3ea17568e6e1306bde4af8_720w.jpg)

3. **合并一次项系数，二次项系数，得到最终目标函数**

![](https://pic2.zhimg.com/80/v2-7179ed952389b97f9cbc3d3dca47765d_720w.jpg)

![](https://pic1.zhimg.com/80/v2-8fbf510ce664cfbb2c67a3cf5c9d6f18_720w.jpg)

![](https://pic4.zhimg.com/80/v2-68d6ca5b15a36f7f4740b93437f7fc9f_720w.jpg)

![](https://pic2.zhimg.com/80/v2-9a3e36faf949d2202bd5b4be5ac6cb95_720w.jpg)

xgboost采用预剪枝策略，只有分裂后的增益大于0才会进行分裂。

- - 参数

  - - XGB架构参数

    - - booster：CART、或者线性模型、或者DART

      - n_estimator：

      - objective：

      - - 分类：MSE
        - 分类：二分类用logistic、多分类用softmax

    - 弱学习器参数

    - - max_depth：树的深度
      - min_child_weight：最小子节点的权重。如果某个子节点权重小于这个阈值，则不会在分裂。使用的是该节点所有二阶导数的和
      - gamma：分裂所带来的损失最小阈值，大于此值，才能继续分裂
      - subsample：子采样参数，无放回抽样
      - colsample_bytree 整棵树的特征采样比例
      - colsample_bylevel 某层的特征采样比例
      - colsample_bynode 某一个树节点的特征采样比例
      - reg_alpha：L1正则化参数
      - reg_lambda： L2正则化参数

- - - 其他

    - - n_jobs控制算法的并发线程数

      - scale_pos_weight用于类别不平衡的时候，负例和正例的比例。类似于sklearn中的class_weight

      - importance_type则可以查询各个特征的重要性程度。最后可以通过调用booster的get_score方法获取对应的特征权重。

      - - “weight”通过特征被选中作为分裂特征的计数来计算重要性
        - “gain”和“total_gain”则通过分别计算特征被选中做分裂特征时带来的平均增益和总增益来计算重要性
        - “cover”和 “total_cover”通过计算特征被选中做分裂时的平均样本覆盖度和总体样本覆盖度来来计算重要性。

### xgboost过拟合后如何调参

1. 直接控制参数的复杂度：max_depth, min_child_weight, gamma
2. 列抽样subsample，训练的时候只用一部分特征，类似randomforest
3. subsample子采样，每轮计算可以不使用全部的样本，是一个0-1之间的ratio

### XGBoost VS GBDT

相同点：都是基于boosting思想的集成学习方法

不同点：

1. GBDT是机器学习算法，XGBoost是工程实现。
2. xgboost采用二阶导优化，GBDT采用一阶导优化；
3. XGBoost的目标函数加入了正则，而GBDT没有；
4. **列抽样**：XGBoost支持列采样，与随机森林类似，用于防止过拟合。
5. XGBoost自动处理缺省值，而GBDT不允许缺失值。通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。
6. **并行化**：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。

### 如何求特征重要性

RF：

1.通过计算Gini系数的减少量 VIm=GI−(GIL+GIR) 判断特征重要性，越大越重要。

GBDT计算方法：所有回归树中通过特征i分裂后平方损失的减少值的和/回归树数量 得到特征重要性。

在sklearn中，GBDT和RF的特征重要性计算方法是相同的，都是基于单棵树计算每个特征的重要性，探究每个特征在每棵树上做了多少的贡献，再取个平均值。

XGBoost主要有三种计算方法：

a. importance_type=weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数。

b. importance_type=gain，特征重要性使用特征在作为划分属性时loss平均的降低量。

c. importance_type=cover，特征重要性使用特征在作为划分属性时对样本的覆盖度。

### XGBoost如何处理缺失值

1. 在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。

2. 在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。

3. 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。

### LightGBM和XGBoost区别？

[LightGBM对比XGBoost](https://blog.csdn.net/weixin_37679531/article/details/105151091#:~:text=LightGBM，与XGBoost一样，都是对GBDT的改进与高效实现，原理上讲，它们都是采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树。 论文地址：LightGBM A,Highly Efficient Gradient Boosting)

## LR vs RF vs XGB

### LR

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高；不能很好地处理大量多类特征或变量

### RF

优点：1. 性能好 2. 能处理高维数据 3. 随机森林算法能处理缺失值 4. 抗过拟合效果比较好 5. 能解决分类和回归两种问题

缺点：1. 解决回归问题表现不如分类问题好，不能给出连续的输出 2. 黑盒，无法控制模型内部的运行，只能在不同参数和随机种子之间尝试 3. 对于小数据或低维数据，不是很好的选择，计算量增大

### XGB

优点：1. 性能好 2. 可以处理缺失值 3. block结构，虽然boosting算法必须串行生成树，但在处理每个特征列时可以并行 4. boosting算法，不易过拟合 5. 可以用于分类和回归问题

# 提升方法

## stacking

为了解决这个泄漏的问题，需要通过 K-Fold 方法分别输出各部分样本的结果，这里以 5-Fold 为例，具体步骤如下

(1) 将数据划分为 5 部分，每次用其中 1 部分做验证集，其余 4 部分做训练集，则共可训练出 5 个模型 (2) 对于训练集，每次训练出一个模型时，通过该模型对没有用来训练的验证集进行预测，将预测结果作为验证集对应的样本的第二层输入，则依次遍历5次后，每个训练样本都可得到其输出结果作为第二层模型的输入 (3) 对于测试集，每次训练出一个模型时，都用这个模型对其进行预测，则最终测试集的每个样本都会有5个输出结果，对这些结果取平均作为该样本的第二层输入

![image-20220210203629139](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220210203629139.png)

## Bagging

基于bootsrap sampling自主采样法，重复性有放回的随机采用部分样本进行训练，最后再将结果voting或者averaging。

并行算法

每个基学习器的未用作训练样本可以用来做包外估计，评价泛化性能。

## Boosting

个体学习器之间存在强依赖，必须串行生成的序列化方法

工作机制：

1. 先从初始训练集训练出一个基学习器
2. 再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注
3. 然后基于调整后的样本分布来训练下一个基学习器
4. 如此重复进行，直至基学习器数目达到事先指定的值T
5. 最终将这T个基学习器进行加权结合

## 方差与偏差

![](https://bbs-img.huaweicloud.com/data/forums/attachment/forum/202103/12/102532verewbwucjxw4wvc.png)

Bagging减小方差，Boosting减小偏差

# 深度学习

## 常用的参数初始化方法

![](https://img-blog.csdn.net/20180408112207951?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L216cG16aw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## Normalization

[详解深度学习中的Normalization，BN/LN/WN - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/33173246)

### Normalization的通用框架

![image-20220223161427730](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220223161427730.png)

$$h$$为某神经元的输出，$$f$$为神经元的计算操作，而括号内的为normalization的变换。

### BN and LN

![Batch Normalization](https://pic1.zhimg.com/80/v2-13bb64b6122e98421ea3528539c1bffc_720w.jpg)

BN(Batch Normalization)

规范化针对单个神经元进行，利用网络训练时一个mini-batch的数据来计算该神经元$$x_i$$的均值与方差

适用场景：每个mini-batch较大，数据分布比较接近。因为若mini-batch较小，则很有可能每个batch的分布差距很大，就加大了模型训练的难度。另外，在训练之前，要进行shuffle，否则效果会差很多。

测试时：

测试时一般是针对单个样本或者少数几个样本，如果针对这几个样本计算的话，会得到有偏估计。**因此训练时，神经网络需要记住每个批次算得的均值与方差**，然后统计在整个训练数据上的统计量用于对测试数据进行标准化：

优点：

- BN即Batch Normalization，可以缓解**internal covariate shift**问题，加速神经网络的训练，保证网络的稳定性。
- BN有正则化作用，可以无需额外使用dropout来避免过拟合，从而提高泛化能力。
- BN对不同的初始化机制和学习率更鲁棒，即降低了对初始化的要求。

**BN为什么要放在激活函数之前？**

一般放在sigmoid，tanh之前，因为这些激活函数会导致梯度消失，在之前做BN可以缓解梯度消失；而在Relu方面，一般放在relu之后，因为relu在正值上是没有上限的，使用BN可以控制在一定的范围内。实际上，放在前面和后面还是都尝试一下取较好结果。

![Layer Normalization](https://pic1.zhimg.com/80/v2-2f1ad5749e4432d11e777cf24b655da8_720w.jpg)

LN(Layer Normalization)

针对BN的不足提出的。与BN不同，LN是一种横向的规范化，综合考虑一层所有维度的输入，计算该层的平均输入值与输入方差，然后用同一个规范化操作来转换各个维度的输入。**LN针对单个训练样本进行**，因此可以避免BN中受mini-batch数据分布影响的问题，可以用于小mini-batch场景，动态网络场景和RNN，特别是NLP领域。

优点：在LN中，一条输入数据，所有神经元的均值和方差都是一样的，不受到批次的约束。并且训练和测试的处理方式是一样的，不用记住训练时的统计量，节省存储空间。

缺点：LN针对同一层的神经元得到同一个转换，所有的输入都在同一个区间范围内，可能会降低模型的表达能力。

## 池化层的作用

1. 不变性(invariance)，平移不变性，旋转不变性，尺度不变性
2. 保留主要的特征同时减少参数，降维；降维的同时也就伴随着参数的减少，可以避免过拟合。

## 1*1卷积核的作用

1. 实现跨通道的交互和信息融合
2. 升维或降维，在多通道的数据上，可以改变通道数，但不修改数据的长宽。
3. 增加非线性。1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep。

## Softmax

$$
\sigma(\mathbf{z})_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K} e^{z_{k}}} \text { for } j=1, \ldots, K
$$

在二分类问题上，softmax可以退化为sigmoid函数

## 计算全连接层、卷积层参数与计算量

[CNN卷积层、全连接层的参数量、计算量 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/77471991)

但注意上面的文章计算计算量时有错误，详见评论区

- 给定一个卷积函数的输入图尺寸、输出图尺寸和卷积核的大小，求卷积操作的步长和填充

## Dropout

dropout只针对训练时，而测试时不再引入dropout

dropout如何平衡训练和测试时的差异？毕竟训练时的输出神经元比较少，所以会除以一个(1-p)，p为失活概率，使得该层的神经元输出有大致相同的期望。

## Dropout和BN在训练和测试时有哪些差别

### Batch Normalization

BN，Batch Normalization，就是在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。

**BN训练和测试时的参数是一样的吗？**

对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。

而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，这个可以通过移动平均法求得。

对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

**BN训练时为什么不用全量训练集的均值和方差呢？**

因为在训练的第一个完整epoch过程中是无法得到输入层之外其他层全量训练集的均值和方差，只能在前向传播过程中获取已训练batch的均值和方差。那在一个完整epoch之后可以使用全量数据集的均值和方差嘛？

对于BN，是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。

但是一批数据和全量数据的均值和方差相差太多，又无法较好地代表训练集的分布，因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，去缩小与全量数据的差别。

### **Dropout**

Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。

**Dropout 在训练和测试时都需要吗？**

Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。

而在测试时，应该用整个训练好的模型，因此不需要dropout。

**Dropout 如何平衡训练和测试时的差异呢？**

Dropout ，在训练时以一定的概率使神经元失活，实际上就是让对应神经元的输出为0

假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。

因此在训练时还要对第二层的输出数据除以（1-p）之后再传给输出层神经元，作为神经元失活的补偿，以使得在训练时和测试时每一层输入有大致相同的期望。

## 序列模型

RNN和LSTM,GRU的对比优缺点

RNN
$$
\begin{gathered}
h^{t}=\sigma\left(W^{h} h^{t-1}+W^{x} x^{t}\right) \\
y^{t}=\sigma\left(W^{y} h^{t}\right)
\end{gathered}
$$
LSTM
$$
\begin{gathered}
i_{t}=\sigma\left(W_{i} X_{t}+U_{i} h_{t-1}+b_{i}\right), \\
f_{t}=\sigma\left(W_{f} X_{t}+U_{f} h_{t-1}+b_{f}\right), \\
o_{t}=\sigma\left(W_{o} X_{t}+U_{o} h_{t-1}+b_{o}\right), \\
\tilde{c}_{t}=\operatorname{Tanh}\left(W_{c} X_{t}+U_{c} h_{t-1}\right), \\
c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t}, \\
h_{t}=o_{t} \odot \operatorname{Tanh}\left(c_{t}\right) .
\end{gathered}
$$
GRU
$$
\begin{gathered}
r=\sigma\left(W^{r}\left[h^{t-1}, x^{t}\right]\right) \\
z=\sigma\left(W^{r}\left[h^{t-1}, x^{t}\right]\right) \\
h^{t-1^{\prime}}=r \odot h^{t-1} \\
h^{\prime}=\tanh \left(W h^{t-1^{\prime}}\right) \\
h^{t}=(1-z) \odot h^{t-1}+z \odot h^{\prime}
\end{gathered}
$$
RNN

对较远时间步的梯度消失，不能长期依赖。

LSTM

通过门控单元，遗忘门参数趋向于1时，可以尽可能保存过去的信息。

GRU

相较于LSTM，GRU只有两个门，参数更少，减少了过拟合的风险。

### LSTM中各模块分别使用什么激活函数，可以使用别的激活函数码？

在LSTM中，遗忘门、输入门、输出门使用Sigmoid函数作为激活函数；在生成候选记忆时，使用双曲正切函数Tanh作为激活函数。值得注意的是，这两个函数都是饱和的，也就是在输入达到一定值的情况下，输出不会发生明显变化。如果是非饱和的激活函数，比如ReLU,那么就难以实现门控的效果。Sigmoid函数的输出在0~1之间，符合门控的物理意义。且当输入较小或较大时，其输出会非常接近0或1，从而保证该门开或关。  在生成候选记忆时，使用Tanh函数，因为其输出在-1~1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh在输入为0的附近比Sigmoid有更大梯度，通常使模型收敛更快。 激活函数选取不是一成不变的，例如在原始LSTM中，使用的是Sigmoid函数的变种，h(x)=2sigmoid(x)−1，g(x)=4sigmoid(x)−2h(x)=2sigmoid(x)-1，g(x)=4sigmoid(x)-2h(x)=2sigmoid(x)−1，g(x)=4sigmoid(x)−2,这两个函数的范围分别是[-1,1]和[-2,2]。后来发现效果并不如sigmoid。  实际上在门控中，使用Sigmoid几乎是现代所有神经网络模块的共同选择。例如在GRU和Attention机制中，也采用Sigmoid作为门控激活函数。  在计算能力有限制的设备中，由于Sigmoid函数求指数需要一定的计算量，此时会使用0/1门控输出为0或1的离散值。即当输入小于阈值，门控输出为0；大于阈值时，输出为1。

![img](https://img-blog.csdnimg.cn/20200621203429662.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poNTE1ODU4MjM3,size_16,color_FFFFFF,t_70)

为什么说lstm不能很好地pre-train，是因为每句话都有不同的风格，且lstm其实存储的空间也是有限的；而transformer的注意力大小是无限的，所以训练好的transformer模型可以简单地微调。

## Transformer

[Transformer面试题](https://zhuanlan.zhihu.com/p/396499248)

![](https://image.jiqizhixin.com/uploads/editor/89a17e0b-0513-4f90-9391-466bfd33a8f4/image__6_.png)

![img](https://pic2.zhimg.com/80/v2-0383a449a3192a903111f41ebc04aba9_720w.jpg)

**self-attention**

---

### self-attention中的query，key，value分别是什么？

self-attention 之所以取推荐系统中的 query、key 、value三个概念，就是利用了与推荐系统相似的流程。但是 self-attention 不是为了 query 去找 value，而是根据当前 query 获取 value 的加权和。这是 self-attention 的任务使然，想要为当前输入找到一个更好的加权输出，该输出要包含所有可见的输入序列信息，而注意力就是通过权重来控制。在电影推荐中。query 是某个人对电影的喜好信息（比如兴趣点、年龄、性别等）、key 是电影的类型（喜剧、年代等）、value 就是待推荐的电影。

query与key作的为**点积相似度**，两向量直接点乘。

深度学习attention机制中的Q,K,V分别是从哪来的？ - lllltdaf的回答 - 知乎 https://www.zhihu.com/question/325839123/answer/1903376265

### Transformer架构图

[transformer - NoteBook (gitbook.io)](https://jia-pengyue.gitbook.io/notebook/machine-learning/papers/transformer)

### 多头注意力机制是什么

Multi-Head Attention相当于$$h$$个不同的self-attention的集成（ensemble)

![](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20211229132023159.png)

先将V,K,Q都投影到较低维度，经过注意力函数后聚合。

为什么要这样做呢，因为单使用Scaled Dot-Product Attention，没有什么可以学习的参数，所以引入一些线性层可以学习一些参数用来捕获不同的模式。

### Slef-attention的乘法计算和加法计算有什么区别？什么时候乘比较好，什么时候加？为什么要除以一个根号？ 

矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的模型效果越显著。

为什么除以一个根号：

随着q，k的向量维度的增长，点积模型的值通常有比较大方差，即部分值很大部分值很小这样较为极端的情况,softmax本身对于值之间的相对大小有很强的放大作用, 即计算出的权重都集中在0和1两个极端上了，因此需要缩放点积模型，适当缩小权重矩阵。

## GAN

分为生成器与判别器，两者是对抗关系。在训练时往往是交替训练的，固定判别器，优化生成器；再固定生成器，优化判别器。

## 常用优化器

### BGD

Batch Gradient Descent，批量梯度下降。根据整个训练集梯度进行梯度下降。

优点：

当损失函数是凸函数时，BGD能收敛到全局最优；当损失函数非凸时，BGD能收敛到局部最优

缺点：

1. 每次根据全部的数据来计算梯度，速度比较慢
2. BGD不能够在线训练，不能根据新数据来实时更新模型

### SGD

Stochastic Gradient Descent，随机梯度下降。每次只使用一个训练样本来进行梯度更新

优点：

1. SGD每次只根据一个样本计算梯度，速度较快
2. SGD可以根据新样本实时更新模型

缺点：

1. SGD在优化的过程中损失的震荡会比较严重

### MBGD

Mini-batch Gradient Descent，小批量梯度下降。MBGD是BGD和SGD的折中。

优点：

1. 收敛更加稳定
2. 可以利用高度优化的矩阵库加速计算过程

缺点：

1. 选择合适的学习率比较难
2. 相同的学习率被应用到了所有的参数
3. 容易被困在鞍点(saddle point)

### 指数加权平均

$$
v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}, t>1
$$

$$\beta$$为0.9时，该式约等于前10个数据的均值；$$\beta$$为0.5时，约等于前2个数据的均值

### Momentum

修改自SGD

![](http://imgtec.eetrend.com/sites/imgtec.eetrend.com/files/201809/blog/17649-36485-1.jpg)
$$
\begin{gathered}
v_{t}=\beta v_{t-1}+(1-\beta) \Delta J(\theta) \\
\theta=\theta-\alpha v_{t}
\end{gathered}
$$
假设模型在时间t的梯度为$$\Delta J(\theta)$$，其中，$$v_t$$为前$$\frac{1}{1-\beta}$$步梯度的均值，$$\alpha$$为学习率。

优点：

利用指数加权平均使得梯度下降的过程更加平滑，减少了震荡，加快收敛

### NAG

Nesterov Accelerated Gradient，对Momentum进行了轻微的修改。

![](http://imgtec.eetrend.com/sites/imgtec.eetrend.com/files/201809/blog/17649-36486-2.jpg)
$$
\begin{gathered}
v_{t}=\beta v_{t-1}+(1-\beta) \Delta J\left(\theta-\beta v_{t-1}\right) \\
\theta=\theta-\alpha v_{t}
\end{gathered}
$$
![image-20220211192335907](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220211192335907.png)

这个式子比较好理解，相当于我们知道不论如何，梯度一定会减去$$\alpha * \beta * d_{i-1}$$，那么我们也可以先让$$\theta$$更新这部分，再在新的位置计算梯度用于更新。在更超前的位置查看梯度，判断更新方向。

深层来说，NAG相当于考虑了二阶导，利用二阶导的信息调整梯度的增减。

### Adagrad

想让训练开始时学习率较大，训练一段时间后学习率逐渐减小
$$
\theta=\theta-\frac{\eta}{\sqrt{\sum_{t} g_{t}^{2}+\epsilon}} g_{t}
$$
![image-20220211193759193](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220211193759193.png)

优点：

1. 自动调整参数的学习率

缺点：

1. 学习率下降比较快，可能造成学习提早停止

### Adadelta

对Adagrad做了轻微的修改，使Adagrad更加稳定
$$
\begin{aligned}
\theta &=\theta-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t} \\
E\left[g_{t}^{2}\right] &=\beta E\left[g^{2}\right]_{t-1}+(1-\beta) g_{t}^{2}
\end{aligned}
$$
其实就是把Adagrad中的分母的梯度平方和换成了梯度平方的指数加权平均，使得Adadelta学习率的下降速度没有Adagrad那么快

### RMSprop

Root Mean Squre propogation，是Adadelta的一个特例

在梯度平方的指数加权平方时，选用参数$$\beta=0.5$$，再求根，就变成了RMS，相当于将每个梯度平方的系数都取0.5，所以是平均的。
$$
\begin{gathered}
R M S[g]_{t}=\sqrt{E\left[g^{2}\right]_{t}+\epsilon} \\
\theta=\theta-\frac{\eta}{R M S[g]_{t}} g_{t}
\end{gathered}
$$

### Adam

Adaptive Moment Estimation，可以看作是Momentum + RMSprop

Adam使用梯度的指数加权平均，用梯度平方的指数加权平均动态调整学习率
$$
\begin{aligned}
m_{t} &=\beta m_{t-1}+(1-\beta) g_{t} \\
n_{t} &=\gamma n_{t-1}+(1-\gamma) g_{t}^{2}
\end{aligned}
$$

中间要引入偏差修正，比如在训练开始时，前面还没有足够的数量的数据用于平均，若直接初始化为0则整个梯度会很小，这时候需要偏差修正。在t很小时，分母可能为0.1，0.2，将$$m_t,n_t$$相对放大，而随着t的增大，分母会越来越接近于1。
$$
\begin{aligned}
\hat{m}_{t} &=\frac{m_{t}}{1-\beta^{t}} \\
\hat{n}_{t} &=\frac{n_{t}}{1-\gamma^{t}}
\end{aligned}
$$


$$
\theta=\theta-\frac{\eta}{\sqrt{\hat{n}_{t}}+\epsilon} \hat{m}_{t}
$$

## 激活函数优缺点

### sigmoid

$$
g(s)=\sigma(s)=\frac{1}{1+e^{-s}} \quad \rightarrow g^{\prime}(s)= (1-g(s))g(s)
$$

缺点：

1. 梯度消失
2. sigmoid outputs are not zero-centered，均为同一个方向
3. exp() is a bit expensive to compute

### Tanh

$$
g(s)=\frac{2}{1+e^{-2 s}}-1=\frac{1-e^{-2 s}}{1+e^{-2 s}} \quad \rightarrow g^{\prime}(s)=1-g(s)^2
$$

优点：

1. zero-centered

缺点：

1. 梯度消失

### Relu

$$
g(s)=\left\{\begin{array}{ll}
0 & \text { if } s<0 \\
s & \text { if } s \geq 0
\end{array} \quad \rightarrow g^{\prime}(s)= \begin{cases}0 & \text { if } s<0 \\
1 & \text { if } s \geq 0\end{cases}\right.
$$

优点：

1. 在值为正值时不会产生梯度消失，但在负值时仍可能发生，负值的梯度为0
2. converge 相比sigmoid/tanh更快
3. 计算高效

缺点：

1. None-zero centered

## 分类问题常用损失函数

[常用的分类问题中的损失函数_wait1ess的博客-CSDN博客_分类损失函数](https://blog.csdn.net/weixin_41065383/article/details/89413819)

# Pytorch

## 如何实现交叉熵

`nn.CrossEntropyLoss()`

## 如何引入正则

1. 通过torch.optim优化器实现L2正则，设置参数weight_decay即L2正则化的$$\lambda$$参数
2. 实现L1正则只能手动实现

```python
regularization_loss = 0
for param in model.parameters():
    regularization_loss += torch.sum(abs(param))

calssify_loss = criterion(pred,target)
loss = classify_loss + lamda * regularization_loss

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

## 实现LR

```python
class LogisticRegression(nn.Module):
    def __init__(self):
        super(LogisticRegression,self).__init__()
        self.lr=nn.Linear(2,1)
        self.sm=nn.Sigmoid()

    def forward(self, x):
        x=self.lr(x)
        x=self.sm(x)
        return x
```

## 维度操作

1. torch.cat 在指定的维度上进行连接操作，所有tensor必须有相同的size
2. torch.stack 增加新的维度进行堆叠
3. torch.squeeze 将维度为1的维度删去
4. torch.tensor.view 返回有相同数据但形状不同的张量
5. torch.tensor.reshape 功能与view差不多，但更强
6. torch.tensor.permute 执行张量的维度换位

# Python

## 装饰器

## 迭代器与生成器

迭代器：`__iter__` `__next__`，iter函数返回self，next函数返回已经遍历过的数据counter，若counter达到迭代器长度，则抛出`stopiteration`的异常

在一次for循环中，首先执行迭代器对象的`__iter__`获取返回值，然后一直反复执行next方法

生成器是一种特殊的迭代器，关键字`yield`

```python
def fab(max): 
    n, a, b = 0, 0, 1 
    while n < max: 
        yield b      # 使用 yield
        # print b 
        a, b = b, a + b 
        n = n + 1
 
for n in fab(5): 
    print n
```

yield 的作用就是把一个函数变成一个 generator，带有 yield 的函数不再是一个普通函数，Python 解释器会将其视为一个 generator，调用 fab(5) 不会执行 fab 函数，而是返回一个 iterable 对象！在 for 循环执行时，每次循环都会执行 fab 函数内部的代码，执行到 yield b 时，fab 函数就返回一个迭代值，下次迭代时，代码从 yield b 的下一条语句继续执行，而函数的本地变量看起来和上次中断执行前是完全一样的，于是函数继续执行，直到再次遇到 yield。

也可以手动调用 fab(5) 的 next() 方法（因为 fab(5) 是一个 generator 对象，该对象具有 next() 方法），这样我们就可以更清楚地看到 fab 的执行流程

可迭代对象：若有一个`__iter__`函数，且该函数的返回值是一个迭代器对象，则称为可迭代对象，常用来对迭代器作抽象

## 进程与线程

一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。

有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。

## 深拷贝与浅拷贝

[Python 直接赋值、浅拷贝和深度拷贝解析 | 菜鸟教程 (runoob.com)](https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html)