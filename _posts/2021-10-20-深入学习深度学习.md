---
layout:     post
title:      "Dive Into Deep Learning"
subtitle:   "动手学深度学习"
date:       2021-10-20 18:00:00
author:     "Jpy"
header-img: "img/deeplearning.png"
tags:
    - Machine Learning
    - Pytorch
    - CS
    - Deep Learning
---

# 前言

深入学习深度学习是李沐博士出版的书，最近偶然发现出了直播教程，正好最近的项目需要用到神经网络，所以跟着学习了一番。

# Week1

数据中的类别值或离散值，可以将`NaN`视为一个类别

```python
inputs = pandas.get_dummies(inputs,dummy_na = True)
```

Tensor的reshape是不会更换地址的

```python
a = torch.arange(12)
b = a.reshape((3,4))
b[:] = 2
a # 这里打印出来的a也是被改为元素均为2
```

神经网络的正向反向的复杂度其实是差不多的。

**如何存梯度以及读取梯度**

```python
x.requires_grad_(True)
x.grad.zero_() # 在下一次计算时要把梯度清零
y = 关于x的计算
y.backward()
x.grad # 访问梯度
```

# Week2

线性回归问题，线性模型可以看作是单层的神经网络

优化时，深度学习比较常用小批量随机梯度下降。因为在整个训练集上算梯度花费太高。因此可以随机采样一些样本计算近似损失。

线性回归notebook：https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html

Softmax回归

回归和分类有较大的不同。分类问题通常有多个输出，输出i是预测第i类的置信度。

sofemax可以把正负值转化为概率，即预测为某一类的概率，相当于**多分类**。

# Week3

## 感知机

感觉是经典的神经网络结构？

output = wx+b

**多层感知机**：可以弥补单层感知机的缺陷，如感知机不能处理XOR分类问题。

## 为什么需要非线性激活函数？

因为没有激活函数，多层的感知机还是线性的变换，其实依然相当于一个单层的线性模型。

常用激活函数：Sigmoid ReLU Tanh

ReLU(x) = max(x,0)，为什么很多时候用relu，因为很简单，相比其他激活函数不用用到指数运算。

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-18_13-34-18.jpg)

**一层隐藏层理论上可以拟合任何函数，为什么不是去增加隐藏层的神经元而是去增加层数呢？**因为，单层很多神经元的网络结构不容易训练。而多层较少神经元的网络结构比较容易训练。

## 模型选择

训练误差：模型在**训练数据**上的误差

泛化误差：模型在**新数据**上的误差**（我们关心的）**

验证数据集：一个用来评估模型好坏的数据集，可以调参。

测试数据集：只用一次的数据集，不能用来调参。

K-则交叉验证：在没有足够多的数据时使用（常态）。将数据分割为K块，使用第i块作为验证数据集，其余的作为训练数据集。

## 过拟合与欠拟合

在简单数据集上应用复杂（高容量）模型，容易过拟合。

在复杂数据集上应用简单（低容量）模型，容易欠拟合。

模型的容量指的是模型拟合函数的能力。

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-18_14-53-44.jpg)

## QA

1. 超参数的设计？经验；比较推荐随机方法选超参数
2. 数据不均衡时如何选择训练数据集、验证集？验证集尽量保证是均衡分布的

# week4

## 权重衰退

通过限制w权重始终小于一个值(硬性限制，每一个w都小于某一个值)

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_10-54-23.jpg)

与加入正则项的作用很相似(柔性的限制)

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_10-54-55.jpg)

*为什么叫权重衰退*：每次在更新参数时，都先给（1-λn）w作一次缩小。λ是我们用来控制模型复杂度的超参数。

## QA

1. 为什么参数小就代表复杂度小呢？其实这里不是说参数小就复杂度小，只是在模型选取参数时，给它限制了参数选择的范围，使得模型的空间相对不作限制小了很多。
2. 一般权重衰退的值设置多少为好呢？一般取0.001

## Dropout

dropout有人说相当于一个正则。

对x加入噪音后，希望其期望不变，依然是x。

丢弃法对每个元素进行如下的扰动，将一些输出项随机置0来控制模型复杂度，常用于多层感知机的隐藏层输出上。

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_11-50-17.jpg)

那么在这时，期望E = P*0 + (1-P)\*(x/(1-P)) = x

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_11-53-26.jpg)

# 数值稳定性

梯度爆炸与梯度消失，因为链式法则会有很多次乘法。

梯度爆炸的问题：

* 梯度超出值域（一般用的16位浮点数）
* 对学习率敏感

梯度消失的问题：

* 梯度值变为0，训练无进展
* 使训练只对顶层训练较好，对底部层没有效果

## 如何让训练更稳定

**合理的权重初始化和激活函数的选取**

将乘法作加法，如ResNet,LSTM

Xavier初始化方法，得知该层的输入输出维度，可以确定初始化参数，相对保证权重的期望为0，方差固定。

要完成这个目标，激活函数如
$$
\begin{equation}\sigma(x)=\alpha x+\beta\end{equation}
$$
也需要满足α=1，β=0

可以看到常用的激活函数是符合这个规律的
$$
\begin{equation}\tanh (x)=0+x-\frac{x^{3}}{3}+O\left(x^{5}\right)\end{equation}
$$

$$
\begin{equation}\operatorname{relu}(x)=0+x \quad$ for $x \geq 0\end{equation}
$$

sigmoid 需要经过一点变换
$$
\begin{equation}\operatorname{sigmoid}(x)=\frac{1}{2}+\frac{x}{4}-\frac{x^{3}}{48}+O\left(x^{5}\right)\end{equation}
$$

$$
\begin{equation}4 \times \operatorname{sigmoid}(x)-2\end{equation}
$$

# PyTorch基础

pytorch中任何一个层，一个模型都是nn.Module的子类

取权重状态可以用`net.state_dict()`，偏移可以用`net.bias`

# 卷积层

pytorch中卷积层的简单调用

```python
conv2d = nn.Conv2d(1,1,kernel_size=(1,2),bias=False) # 参数为输入通道数、输出通道数、卷积核大小，偏移
input = X.reshape((1,1,6,8))# 形状为通道数，batchsize，长，宽
output = Y.reshape((1,1,6,7))
```

## 卷积层里的填充Padding和步幅Stride

填充 （针对输入的填充）

可以使得卷积之后的输出，维度更大一些，可以使用更大的卷积核，更深的网络。

![image-20211020140530712](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20211020140530712.png)

如图所示，蓝框所标注的为原始的输入，经过填充一圈0后，使得卷积后输出比原始输入维度还大。

一般会取`padding_height = kernel_height - 1, padding_width = kernel_width - 1`，如此输出的维度会与原始输入维度一致。

步幅

可以控制卷积核每次移动的步幅，如高度3宽度2的步幅。

**可以说，填充是为了尽量延长卷积的过程，而步幅是尽量加快卷积的过程。**

```python
# pytorch代码实现
# 通过padding参数控制填充
# 通过stride参数控制步幅
nn.Conv2d(1,1,kernel_size=3,padding=(0,1), stride=(2,3))
```

## 卷积层里的多输入多输出通道

每个通道都有一个卷积核，结果是所有通道卷积结果的和

![](https://github.com/Jia-py/blog_picture/blob/master/img/image-20211020142510632.png?raw=true)

这样得到的输出通道是单通道

如何获得多输出通道呢？只需要设置n组kernel即可，即设置多个输出单输出通道的卷积核组，这样就可以输出多个输出通道了。

为什么要多输入多输出通道呢？因为每个不同的通道可以识别不同的模式，比如一个通道识别猫耳朵，一个通道识别猫眼。多个模式可以`加权`之后再作为下一个卷积层的多通道输入。

### 1X1卷积层

1x1卷积核不识别空间模式，只是用来融合通道。即上面提到的`加权`操作

# 池化层 Pooling

起因是我们需要一定程度的平移不变性，如卷积层在边缘检测中对图像边界太过敏感。

## 二维最大池化层

如2x2 max pooling， 即在这四个格子中找最大的值输出。与卷积相比，没有了做积的概念。

另外还有平均池化层

```python
pool2d = nn.MaxPool2d(n)
# pytorch默认步幅和窗口大小是一样的，也就是池化的窗口是不会重叠的
# 当然，也可以手动指定
pool2d = nn.MaxPool2d((2,3), padding=(1,1), stride=(2,3))
```

# LeNet

主要提出是用于手写数字识别

结构图

![image-20211020164650170](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20211020164650170.png)

pytorch实现

```python
net = torch.nn.Sequential(
	nn.Conv2d(1,6,kernel_size=5,padding=2),nn.Sigmoid(),
    nn.AvgPool2d(2,stride=2),
    nn.Conv2d(6,16,kernel_size=5),nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2,stride=2),
    nn.Flatten(),# 保持第一维，后面的维度全部拉成一个向量，为了输入后续的MLP
    nn.Linear(16*5*5,120),nn.Sigmoid(),
    nn.Linear(120,84),nn.Sigmoid(),
    nn.Linear(84,10)
)

for layer in net:
    X = layer(X)
```

# AlexNet

1980 - 1990 神经网络

2000 - 2010 核方法

2010 - 2020 神经网络

一般神经网络兴起的时间段代表计算能力足够支撑当时的数据量。

相比LeNet，给了人们新的计算机视觉的方法论，量变可以引起质变。

构建了端到端的模型，原始的就是一个图像的pixel，一句话的字符串等等，不需要进行人工的特征提取，输出就是我们要预测的分类等等。

相比lenet，Alexnet更大更深，有大约10倍的参数个数，260倍的计算复杂度，新加入了丢弃法，ReLU，maxpooling以及数据增强。

AlexNet赢下2012ImageNet竞赛后，引发了新的一轮神经网络热潮。

# VGG

相比AlexNet更深更大，更多的FC，更多的卷积层，将卷积层组合成块。

# NiN(Network in Network)

一个NiN块，是一个卷积层+两个全连接层。

# GoogLeNet

Inception块，综合前面的模型，不同大小的卷积，maxpooling都要有。

![image-20220102214447470](https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220102214447470.png)

堆叠了有9个inception块

对比AlexNet，可以看到都是尽可能快地增大通道数，降低图像宽高

# Batch Norm

思想不新，但层是比较新的

一般来说，越靠近input，梯度会越来越小，而越靠近loss function，梯度会比较大。所以后层会训练地比较快，而前层会训练地比较慢，而前层训练后，后层又得重新训练了。

批量归一化

---

1. 固定小批量里面的均值和方差，使得前后层梯度都比较稳定。

   <img src="https://raw.githubusercontent.com/Jia-py/blog_picture/master/img/image-20220102222632599.png" alt="image-20220102222632599" style="zoom:50%;" />

   1. 可学习的参数为$$\gamma$$以及$$\beta$$，相当于缩放和偏移
   2. 一般作用在全连接层和卷积层输出上，激活函数前
   3. 对全连接层，作用在特征维度
   4. 对卷积层，作用在通道维

一般可以加速收敛，但一般不改变模型精度

