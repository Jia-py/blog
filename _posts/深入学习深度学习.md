# 前言

深入学习深度学习是李沐博士出版的书，最近偶然发现出了直播教程，正好最近的项目需要用到神经网络，所以跟着学习了一番。

# Week1

数据中的类别值或离散值，可以将`NaN`视为一个类别

```python
inputs = pandas.get_dummies(inputs,dummy_na = True)
```

Tensor的reshape是不会更换地址的

```python
a = torch.arange(12)
b = a.reshape((3,4))
b[:] = 2
a # 这里打印出来的a也是被改为元素均为2
```

神经网络的正向反向的复杂度其实是差不多的。

**如何存梯度以及读取梯度**

```python
x.requires_grad_(True)
x.grad.zero_() # 在下一次计算时要把梯度清零
y = 关于x的计算
y.backward()
x.grad # 访问梯度
```

# Week2

线性回归问题，线性模型可以看作是单层的神经网络

优化时，深度学习比较常用小批量随机梯度下降。因为在整个训练集上算梯度花费太高。因此可以随机采样一些样本计算近似损失。

线性回归notebook：https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html

Softmax回归

回归和分类有较大的不同。分类问题通常有多个输出，输出i是预测第i类的置信度。

sofemax可以把正负值转化为概率，即预测为某一类的概率。



